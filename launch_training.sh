#!/bin/bash

# Script para lanzar entrenamiento distribuido de Mistral 7B con 8 GPUs H100
# Uso: bash launch_training.sh

echo "ğŸš€ Iniciando entrenamiento distribuido Mistral 7B - frIdA"
echo "ğŸ”§ ConfiguraciÃ³n: 8x H100 GPUs con DeepSpeed ZeRO Stage 2"
echo "=" * 60

# Paso 1: Crear y activar entorno virtual
echo "ğŸ Configurando entorno virtual..."
if [ ! -d "venv" ]; then
    echo "ğŸ“¦ Creando entorno virtual..."
    python -m venv venv
    if [ $? -ne 0 ]; then
        echo "âŒ Error: No se pudo crear el entorno virtual"
        echo "ğŸ’¡ AsegÃºrate de tener python-venv instalado: sudo apt install python3-venv"
        exit 1
    fi
fi

# Activar entorno virtual
echo "ğŸ”„ Activando entorno virtual..."
source venv/bin/activate
if [ $? -ne 0 ]; then
    echo "âŒ Error: No se pudo activar el entorno virtual"
    exit 1
fi

echo "âœ… Entorno virtual activado: $(which python)"

# Paso 2: Verificar e instalar dependencias
echo "ğŸ“¦ Verificando dependencias..."
if ! command -v python &> /dev/null; then
    echo "âŒ Error: Python no estÃ¡ disponible en el entorno virtual"
    exit 1
fi

# Actualizar pip
echo "ğŸ”„ Actualizando pip..."
pip install --upgrade pip

# Verificar si torch estÃ¡ instalado
if ! python -c "import torch" &> /dev/null; then
    echo "âš ï¸  Instalando dependencias..."
    pip install -r requirements.txt
else
    echo "âœ… PyTorch ya estÃ¡ instalado"
    # Verificar otras dependencias crÃ­ticas
    if ! python -c "import transformers, datasets, peft, deepspeed" &> /dev/null; then
        echo "âš ï¸  Instalando dependencias faltantes..."
        pip install -r requirements.txt
    fi
fi

# Paso 3: Descargar modelo Mistral 7B Instruct si no existe
echo "ğŸ“¥ Verificando modelo Mistral 7B Instruct..."
if [ ! -d "~/.cache/huggingface/hub" ] || ! python -c "from transformers import AutoTokenizer; AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2')" &> /dev/null; then
    echo "ğŸ“¥ Descargando modelo Mistral 7B Instruct..."
    python -c "
from transformers import AutoTokenizer, AutoModelForCausalLM
print('Descargando tokenizer...')
tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2')
print('Descargando modelo...')
model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', torch_dtype='auto')
print('âœ… Modelo descargado exitosamente')
"
else
    echo "âœ… Modelo Mistral ya estÃ¡ disponible"
fi

# Paso 4: Generar datos de entrenamiento desde synth
echo "ğŸ”„ Generando datos de entrenamiento..."
if [ -d "synth" ] && [ -f "synth/format.py" ]; then
    cd synth
    echo "ğŸ“ Ejecutando format.py..."
    python format.py
    
    if [ -f "formatted_data.jsonl" ]; then
        echo "âœ… Datos formateados generados exitosamente"
        # Mover el archivo al directorio principal
        mv formatted_data.jsonl ../training_data.jsonl
        echo "ğŸ“ Archivo movido a training_data.jsonl"
    else
        echo "âŒ Error: No se generÃ³ formatted_data.jsonl"
        cd ..
        exit 1
    fi
    cd ..
else
    echo "âŒ Error: No se encontrÃ³ la carpeta synth o format.py"
    exit 1
fi

# Paso 5: Verificar que el archivo de datos existe
if [ ! -f "training_data.jsonl" ]; then
    echo "âŒ Error: No se encontrÃ³ training_data.jsonl despuÃ©s de la generaciÃ³n"
    exit 1
fi

# Verificar formato del archivo
echo "ğŸ” Verificando formato de datos..."
first_line=$(head -n 1 training_data.jsonl)
if echo "$first_line" | grep -q '"text".*<s>\[INST\].*\[/INST\].*</s>'; then
    echo "âœ… Formato de datos correcto"
else
    echo "âš ï¸  Advertencia: El formato de datos podrÃ­a no ser el esperado"
    echo "Primera lÃ­nea: $first_line"
fi

# Mostrar estadÃ­sticas del dataset
num_lines=$(wc -l < training_data.jsonl)
echo "ğŸ“Š Dataset contiene $num_lines ejemplos de entrenamiento"

# Paso 6: Configurar variables de entorno para optimizaciÃ³n
echo "âš™ï¸  Configurando entorno para entrenamiento distribuido..."
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
export TOKENIZERS_PARALLELISM=false
export NCCL_DEBUG=INFO
export NCCL_TREE_THRESHOLD=0
export OMP_NUM_THREADS=8
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1

# Verificar GPUs disponibles
echo "ğŸ” Verificando GPUs disponibles..."
if command -v nvidia-smi &> /dev/null; then
    gpu_count=$(nvidia-smi --list-gpus | wc -l)
    echo "âœ… Detectadas $gpu_count GPUs"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits
else
    echo "âš ï¸  nvidia-smi no disponible, continuando..."
fi

# Paso 7: Lanzar entrenamiento con DeepSpeed
echo "ğŸš€ Iniciando entrenamiento distribuido..."
echo "ğŸ“Š ConfiguraciÃ³n:"
echo "  - Modelo: Mistral 7B Instruct"
echo "  - GPUs: 8x H100"
echo "  - Batch size efectivo: 128"
echo "  - DeepSpeed ZeRO Stage 2"
echo "  - Precision: BFloat16"
echo ""

# Crear directorio de logs si no existe
mkdir -p logs

# Lanzar entrenamiento con logging
deepspeed --num_gpus=8 \
    --master_port=29500 \
    main.py \
    --deepspeed ds_config.json 2>&1 | tee logs/training_$(date +%Y%m%d_%H%M%S).log

# Verificar si el entrenamiento fue exitoso
if [ $? -eq 0 ]; then
    echo "âœ… Entrenamiento completado exitosamente!"
    echo "ğŸ“ Modelo guardado en: ./mistral_7b_frida_finetuned"
    echo "ğŸ“Š Logs guardados en: logs/"
    
    # Mostrar informaciÃ³n del modelo entrenado
    if [ -d "./mistral_7b_frida_finetuned" ]; then
        model_size=$(du -sh ./mistral_7b_frida_finetuned | cut -f1)
        echo "ğŸ’¾ TamaÃ±o del modelo: $model_size"
    fi
else
    echo "âŒ Error durante el entrenamiento"
    echo "ğŸ“‹ Revisa los logs en: logs/"
    exit 1
fi
