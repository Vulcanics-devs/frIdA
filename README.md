# ğŸ”¥ Fine-tuning Mistral 7B Instruct - frIdA

Script optimizado para fine-tuning de Mistral 7B Instruct con 8 GPUs H100 usando DeepSpeed y LoRA.

## ğŸš€ CaracterÃ­sticas

- **Modelo**: Mistral 7B Instruct v0.2
- **Hardware**: Optimizado para 8x H100 GPUs
- **TÃ©cnicas**: LoRA + DeepSpeed ZeRO Stage 2
- **PrecisiÃ³n**: BFloat16 para mÃ¡ximo rendimiento
- **Batch Size Efectivo**: 128 (8 GPUs Ã— 8 batch Ã— 2 accumulation)

## ğŸ“‹ Requisitos

- Python 3.8+
- CUDA 11.8+
- 8x H100 GPUs (mÃ­nimo 80GB VRAM cada una)
- ~200GB espacio libre en disco

## ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio**:
```bash
git clone <tu-repo>
cd frida
```

2. **Crear entorno virtual** (recomendado):
```bash
python -m venv venv
source venv/bin/activate  # En Linux/Mac
# o
venv\Scripts\activate     # En Windows
```

3. **Instalar dependencias**:
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

> **Nota**: El script `launch_training.sh` crea automÃ¡ticamente el entorno virtual si no existe.

## ğŸ“Š Estructura de Datos

El script espera datos en formato JSONL con la estructura de Mistral:

```json
{"text": "<s>[INST] pregunta del usuario [/INST] respuesta del asistente </s>"}
```

## ğŸš€ Uso

### Entrenamiento AutomÃ¡tico

El script `launch_training.sh` automatiza todo el proceso:

```bash
bash launch_training.sh
```

Este script:
1. ğŸ Crea y activa entorno virtual automÃ¡ticamente
2. âœ… Verifica e instala dependencias
3. ğŸ“¥ Descarga el modelo Mistral 7B Instruct
4. ğŸ”„ Genera datos de entrenamiento desde `synth/format.py`
5. ğŸš€ Inicia el entrenamiento distribuido
6. ğŸ’¾ Guarda el modelo entrenado

### Entrenamiento Manual

Si prefieres ejecutar manualmente:

```bash
# 1. Crear y activar entorno virtual
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 2. Generar datos
cd synth
python format.py
mv formatted_data.jsonl ../training_data.jsonl
cd ..

# 3. Entrenar (bÃ¡sico)
deepspeed --num_gpus=8 main.py --deepspeed ds_config.json

# 3. Entrenar (con opciones personalizadas)
deepspeed --num_gpus=8 main.py --deepspeed ds_config.json \
  --epochs 5 \
  --batch-size 4 \
  --learning-rate 2e-4 \
  --data mi_dataset.jsonl \
  --output ./mi_modelo_entrenado
```

### Opciones de LÃ­nea de Comandos

| OpciÃ³n | Predeterminado | DescripciÃ³n |
|--------|----------------|-------------|
| `--model` | `mistralai/Mistral-7B-Instruct-v0.2` | Modelo base a usar |
| `--data` | `training_data.jsonl` | Archivo de datos de entrenamiento |
| `--output` | `./mistral_7b_frida_finetuned` | Directorio de salida |
| `--epochs` | Auto-calculado | NÃºmero de Ã©pocas (sobrescribe cÃ¡lculo) |
| `--batch-size` | `8` | Batch size por dispositivo |
| `--learning-rate` | `1e-4` | Learning rate |

### LÃ³gica de Ã‰pocas AutomÃ¡tica

El script calcula automÃ¡ticamente el nÃºmero Ã³ptimo de Ã©pocas segÃºn el tamaÃ±o del dataset:

| TamaÃ±o del Dataset | Ã‰pocas | RazÃ³n |
|---------------------|--------|--------|
| < 1,000 ejemplos | 5 | Dataset pequeÃ±o, necesita mÃ¡s pasadas |
| 1,000 - 10,000 | 4 | Dataset pequeÃ±o-mediano |
| 10,000 - 50,000 | 3 | Dataset mediano (Ã³ptimo) |
| 50,000 - 100,000 | 2 | Dataset grande |
| > 100,000 | 1 | Dataset muy grande, evita overfitting |

> **Tip**: Puedes sobrescribir con `--epochs N` si tienes necesidades especÃ­ficas.

### Probar el Modelo

DespuÃ©s del entrenamiento, puedes probar el modelo:

```bash
# Activar entorno virtual (si no estÃ¡ activo)
source venv/bin/activate

# Pruebas predefinidas + modo interactivo
python test_model.py

# Solo modo interactivo
python test_model.py --interactive

# Solo pruebas predefinidas
python test_model.py --batch
```

## ğŸ GestiÃ³n del Entorno Virtual

### ActivaciÃ³n RÃ¡pida

Para activar el entorno virtual manualmente:

```bash
# Activar con informaciÃ³n detallada
source activate_env.sh

# O activaciÃ³n simple
source venv/bin/activate
```

### Verificar InstalaciÃ³n

DespuÃ©s de activar el entorno:

```bash
# Verificar versiones
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
python -c "import deepspeed; print(f'DeepSpeed: {deepspeed.__version__}')"

# Verificar GPUs
nvidia-smi
```

### Desactivar Entorno

```bash
deactivate
```

### Recrear Entorno (si hay problemas)

```bash
# Eliminar entorno existente
rm -rf venv

# Crear nuevo entorno
python -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

## âš™ï¸ ConfiguraciÃ³n

### DeepSpeed (ds_config.json)

- **ZeRO Stage 2**: OptimizaciÃ³n de memoria distribuida
- **BFloat16**: PrecisiÃ³n optimizada para H100
- **Gradient Clipping**: Estabilidad de entrenamiento
- **AdamW Optimizer**: Con cosine learning rate schedule

### LoRA Configuration

- **Rank**: 64 (alta calidad)
- **Alpha**: 128
- **Target Modules**: Todas las proyecciones de atenciÃ³n y MLP
- **Dropout**: 0.05

### Training Arguments

- **Epochs**: 3
- **Learning Rate**: 1e-4 con warmup
- **Batch Size**: 8 por GPU
- **Gradient Accumulation**: 2 steps
- **Max Length**: 2048 tokens

## ğŸ“ Estructura del Proyecto

```
frida/
â”œâ”€â”€ main.py                 # Script principal de entrenamiento
â”œâ”€â”€ launch_training.sh      # Script automatizado de lanzamiento
â”œâ”€â”€ activate_env.sh         # Script para activar entorno virtual
â”œâ”€â”€ test_model.py          # Script para probar el modelo
â”œâ”€â”€ ds_config.json         # ConfiguraciÃ³n DeepSpeed
â”œâ”€â”€ requirements.txt       # Dependencias Python
â”œâ”€â”€ README.md              # DocumentaciÃ³n
â”œâ”€â”€ venv/                  # Entorno virtual (creado automÃ¡ticamente)
â”œâ”€â”€ synth/                 # Carpeta con datos sintÃ©ticos
â”‚   â”œâ”€â”€ format.py         # Script para formatear datos
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logs/                  # Logs de entrenamiento
â”œâ”€â”€ training_data.jsonl    # Datos de entrenamiento (generado)
â””â”€â”€ mistral_7b_frida_finetuned/  # Modelo entrenado
```

## ğŸ”§ Optimizaciones

### Para H100 GPUs:
- Flash Attention 2
- BFloat16 precision
- TensorFloat-32 (TF32)
- Optimized memory allocation
- NCCL optimizations

### Para Memoria:
- Gradient checkpointing
- DeepSpeed ZeRO Stage 2
- Dynamic padding
- LoRA (Low-Rank Adaptation)

## ğŸ“Š Monitoreo

- **TensorBoard**: `tensorboard --logdir logs/`
- **Logs**: Guardados en `logs/training_YYYYMMDD_HHMMSS.log`
- **GPU Usage**: `nvidia-smi` durante entrenamiento

## ğŸ› Troubleshooting

### Error de Memoria GPU
```bash
# Reducir batch size en main.py
per_device_train_batch_size=4  # En lugar de 8
```

### Error de Conectividad Multi-GPU
```bash
# Verificar NCCL
export NCCL_DEBUG=INFO
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1
```

### Modelo no Descarga
```bash
# Descargar manualmente
python -c "from transformers import AutoTokenizer, AutoModelForCausalLM; AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2'); AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2')"
```

## ğŸ“ˆ Resultados Esperados

Con 8x H100 GPUs:
- **Velocidad**: ~2-3 horas para 3 epochs (depende del dataset)
- **Memoria**: ~60-70GB por GPU
- **Throughput**: ~1000-1500 tokens/segundo

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT.

---

**Â¡Feliz fine-tuning! ğŸš€**
